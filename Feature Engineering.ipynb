{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77feff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1:. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features\n",
    "#to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "#For example, a series of album ratings scaled from 70 to 150 could be min-maxed so that every rating falls on or between 0\n",
    "#and 1, and the proportional distance between data points is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091b79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2:Unit vector technique is different in terms of formula as unit vector's is divide the 2 variable with magnitude\n",
    "# of those two values by using a right angle triangle and min max scaler is d is datapoint d_scaled = (d-dmin)/(dmax-dmin)\n",
    "#Vectors are used in engineering mechanics to represent quantities that have both a magnitude and a direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50481828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3: Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the\n",
    "#dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most\n",
    "#of the information in the large set.\n",
    "#f you have a dataset of pictures of dogs, PCA could find the features that make a dog look like a dog, such as its shape,\n",
    "#size, and color. basically we are removing redundant or non-important feautures or noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf9da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4:Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data\n",
    "#transformation may be linear, as in PCA, but many nonlinear dimensionality reduction techniques also exist.\n",
    "\n",
    "#PCA identifies the axes in the dataset along which the variation in the data is maximal. In essence, these axes are the\n",
    "#“principal components” of the data distribution. When you project the data onto these axes, you get a dataset with possibly\n",
    "#fewer dimensions than the original, but with preserved variance. These new dimensions are your extracted features.\n",
    "\n",
    "#For example, suppose you have a dataset of images. Each pixel in an image could be considered a feature, so a single image\n",
    "#could have thousands or millions of features. PCA could be used to reduce these thousands or millions of features to just\n",
    "#a handful, while preserving as much of the variance (information) in the dataset as possible. These new features could then\n",
    "#be used for further machine learning tasks, such as classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27505c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5:first we will select the features from the dataset it should be decimal or whole number and then we will fit the\n",
    "#data using module skitlearn or the formula and same apply's for the transformation of data, we will not select features like\n",
    "#daytime and other convertable variables to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87be966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6: in terms of stock market prices we will reduce all the features to 1D like if the stock market data is\n",
    "# has 50 features then we will reduce by drawing a slope in which maximum data is captured between each combination of \n",
    "#features and will repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5a4c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer 7:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'data':[1, 5, 10, 15, 20]})\n",
    "scale = MinMaxScaler()\n",
    "c = scale.fit(data[['data']])\n",
    "scale.transform(data[['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0092689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8:PCA connot be done due to dataset not given here we can use upto 5 PCA analysis it could be lesser because we are\n",
    "#given 5 features and we have to reduce it by capturing most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
