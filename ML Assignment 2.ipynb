{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3d7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1:\n",
    "#Underfitting:A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to\n",
    "#capture data complexities. It represents the inability of the model to learn the training data effectively result in poor\n",
    "#performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when \n",
    "#applied to new, unseen examples.\n",
    "#Overfitting:A statistical model is said to be overfitted when the model does not make accurate predictions on testing data.\n",
    "#When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set.\n",
    "#And when testing with test data results in High variance. Then the model does not categorize the data correctly, because\n",
    "#of too many details and noise.\n",
    "#Consequences\n",
    "#Overfitting:An overfit model gives a very low prediction error on training data, but a very high prediction error on test data\n",
    "#Underfitting:An underfit model results in high prediction errors for both training and test data\n",
    "\n",
    "#Reduce Overfitting: Increase training data.\n",
    "#Reduce model complexity.\n",
    "#Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to \n",
    "#increase stoptraining).\n",
    "\n",
    "#Reduce Underfitting:Increase model complexity.\n",
    "#Increase the number of features, performing feature engineering.\n",
    "#Remove noise from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a4b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2:\n",
    "#Early stopping\n",
    "#Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting\n",
    "#the timing right is important; else the model will still not give accurate results.\n",
    "#Pruning\n",
    "#You might identify several features or parameters that impact the final prediction when you build a model. Feature \n",
    "#selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones.\n",
    "#For example, to predict if an image is an animal or human, you can look at various input parameters like face shape, ear \n",
    "#position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\n",
    "#Regularization\n",
    "#Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to\n",
    "#eliminate those factors that do not impact the prediction outcomes by grading features based on importance. For example,\n",
    "#mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting\n",
    "#to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like\n",
    "#population growth and average annual income but a higher penalty value to the average annual temperature of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3:A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to \n",
    "#capture data complexities. It represents the inability of the model to learn the training data effectively result in poor\n",
    "#performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when \n",
    "#applied to new, unseen examples.\n",
    "\n",
    "#Scenarios could be:\n",
    "#Data used for training is not cleaned and contains noise (garbage values) in it.\n",
    "#The model has a high bias.\n",
    "#The size of the training dataset used is not enough.\n",
    "#The model is too simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0a37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4:If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance \n",
    "#condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on\n",
    "#high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between\n",
    "#both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a \n",
    "#tradeoff between bias and variance.\n",
    "#In machine learning, as you try to minimize one component of the error (e.g., bias), the other component (e.g., variance)\n",
    "#tends to increase, and vice versa. Finding the right balance of bias and variance is key to creating an effective and\n",
    "#accurate model, Bias and variance are inversely connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc93d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5:Reasons for Overfitting:\n",
    "#High variance and low bias.\n",
    "#The model is too complex.\n",
    "#The size of the training data.\n",
    "\n",
    "#Reasons for Underfitting\n",
    "#The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "#The input features which is used to train the model is not the adequate representations of underlying factors influencing \n",
    "#the target variable.\n",
    "#The size of the training dataset used is not enough.\n",
    "#Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "#Features are not scaled.\n",
    "\n",
    "#Detection of overfitting is high variance and low bias\n",
    "#Detection of underfitting is low variance and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825729c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6:Bias is simply defined as the inability of the model because of that there is some difference or error occurring \n",
    "#between the model’s predicted value and the actual value.\n",
    "#These differences between actual or expected values and the predicted values are known as error or bias error or error due \n",
    "#to bias. Bias is a systematic error that occurs due to wrong assumptions in the machine learning process. \n",
    "\n",
    "#Variance is the measure of spread in data from its mean position\n",
    "#In machine learning variance is the amount by which the performance of a predictive model changes when it is trained on \n",
    "#different subsets of the training data. More specifically, variance is the variability of the model that how much it is\n",
    "#sensitive to another subset of the training dataset\n",
    "\n",
    "#High Bias Models: High bias models make strong assumptions about the data and tend to oversimplify it. This can lead to \n",
    "#underfitting, where the model does not capture all the relevant patterns in the data12. Examples of high bias models \n",
    "#include Linear Regression, Linear Discriminant Analysis, and Logistic Regression.\n",
    "\n",
    "#High Variance Models: High variance models do not make strong assumptions about the data and can capture complex patterns.\n",
    "#However, they tend to overfit the data, meaning they capture the noise along with the signal12. Examples of high variance\n",
    "#models include k-Nearest Neighbors (k=1), Decision Trees, and Support Vector Machines\n",
    "\n",
    "#In terms of performance, high bias models may perform well on training data but poorly on unseen data because they\n",
    "#oversimplify the data. On the other hand, high variance models may perform exceptionally well on training data but poorly\n",
    "#on unseen data because they overfit to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7:Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a\n",
    "#type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this\n",
    "#context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "\n",
    "#Ridge regression is one of the types of linear regression in which we introduce a small amount of bias, known as Ridge\n",
    "#regression penalty so that we can get better long-term predictions.\n",
    "#In Statistics, it is known as the L-2 norm.\n",
    "#In this technique, the cost function is altered by adding the penalty term (shrinkage term), which multiplies the lambda\n",
    "#with the squared weight of each individual feature\n",
    "\n",
    "#Lasso Regression\n",
    "#Lasso regression is another variant of the regularization technique used to reduce the complexity of the model. It stands\n",
    "#for Least Absolute and Selection Operator.\n",
    "#It is similar to the Ridge Regression except that the penalty term includes the absolute weights instead of a square of \n",
    "#weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
